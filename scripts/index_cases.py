#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∫–µ–π—Å–æ–≤ BizLevel –≤ RAG-—Å–∏—Å—Ç–µ–º—É
–ü–∞—Ä—Å–∏—Ç –∫–µ–π—Å—ã –∏–∑ Markdown —Ñ–∞–π–ª–æ–≤ –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤ –±–∞–∑—É Supabase —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
"""

import os
import re
import json
import time
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path
from dataclasses import dataclass
import hashlib
import uuid

# Supabase –∏ OpenAI
import openai
from supabase import create_client, Client

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
from dotenv import load_dotenv
script_dir = Path(__file__).parent
env_file = script_dir.parent / '.env'
load_dotenv(env_file)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class CaseChunk:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —á–∞–Ω–∫–∞ –∫–µ–π—Å–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
    case_id: int
    chunk_type: str  # metadata, situation, question, solution, insight
    title: str
    content: str
    after_level: int
    skill_name: str
    tags: List[str]
    business_areas: List[str]
    difficulty: str

class CaseIndexer:
    """–ö–ª–∞—Å—Å –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∫–µ–π—Å–æ–≤ –≤ RAG-—Å–∏—Å—Ç–µ–º—É"""
    
    def __init__(self):
        self.openai_client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.supabase: Client = create_client(
            os.getenv("SUPABASE_URL"),
            os.getenv("SUPABASE_SERVICE_ROLE_KEY")
        )
        self.embedding_model = os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-3-small")
        
    def parse_cases_from_markdown(self, file_path: str) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç –∫–µ–π—Å—ã –∏–∑ Markdown —Ñ–∞–π–ª–∞"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        cases = []
        # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –∫–µ–π—Å–∞–º (–∑–∞–≥–æ–ª–æ–≤–∫–∏ ## –ö–µ–π—Å ‚ÑñX:)
        case_sections = re.split(r'\n## –ö–µ–π—Å ‚Ññ(\d+):', content)
        
        for i in range(1, len(case_sections), 2):
            case_id = int(case_sections[i])
            case_content = case_sections[i + 1]
            
            case_data = self._parse_single_case(case_id, case_content)
            if case_data:
                cases.append(case_data)
                logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ –∫–µ–π—Å–∞ ‚Ññ{case_id}: {case_data['title']}")
        
        return cases
    
    def _parse_single_case(self, case_id: int, content: str) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–π –∫–µ–π—Å"""
        lines = content.strip().split('\n')
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–µ–π—Å–∞
        title_match = re.search(r'^"([^"]+)"', lines[0])
        title = title_match.group(1) if title_match else f"–ö–µ–π—Å ‚Ññ{case_id}"
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = self._extract_metadata(content)
        if not metadata:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–µ–π—Å–∞ ‚Ññ{case_id}")
            return None
            
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —Å–µ–∫—Ü–∏–∏
        sections = self._split_into_sections(content)
        
        return {
            'case_id': case_id,
            'title': title,
            'metadata': metadata,
            'sections': sections
        }
    
    def _extract_metadata(self, content: str) -> Optional[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–µ–π—Å–∞"""
        metadata = {}
        
        # –ü–æ—Å–ª–µ —É—Ä–æ–≤–Ω—è
        level_match = re.search(r'–ü–æ—Å–ª–µ —É—Ä–æ–≤–Ω—è:\*\* (\d+)', content)
        if level_match:
            metadata['after_level'] = int(level_match.group(1))
        else:
            return None
            
        # –ù–∞–≤—ã–∫–∏
        skill_match = re.search(r'–ù–∞–≤—ã–∫–∏:\*\* (.+?)(?:\n|$)', content)
        if skill_match:
            skill_text = skill_match.group(1).strip()
            # –£–±–∏—Ä–∞–µ–º —ç–º–æ–¥–∑–∏ –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –Ω–∞–≤—ã–∫–∞
            skill_clean = re.sub(r'[üß†üí∞‚ö°]+\s*', '', skill_text).strip()
            metadata['skill_name'] = skill_clean
        
        # –ó–∞—Ç—Ä–∞–≥–∏–≤–∞–µ–º—ã–µ —É—Ä–æ–∫–∏ (—Ç–µ–≥–∏)
        lessons_section = re.search(r'–ó–∞—Ç—Ä–∞–≥–∏–≤–∞–µ–º—ã–µ —É—Ä–æ–∫–∏:(.*?)(?=\n###|\n##|$)', content, re.DOTALL)
        tags = []
        if lessons_section:
            lessons_text = lessons_section.group(1)
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —É—Ä–æ–∫–æ–≤
            lesson_matches = re.findall(r'–£—Ä–æ–≤–µ–Ω—å \d+: ([^(]+)', lessons_text)
            tags = [lesson.strip() for lesson in lesson_matches]
        
        metadata['tags'] = tags
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ñ–µ—Ä—ã –±–∏–∑–Ω–µ—Å–∞ –ø–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É
        business_areas = self._detect_business_areas(content)
        metadata['business_areas'] = business_areas
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç—å –ø–æ —É—Ä–æ–≤–Ω—é
        level = metadata.get('after_level', 1)
        if level <= 3:
            difficulty = 'beginner'
        elif level <= 7:
            difficulty = 'intermediate'
        else:
            difficulty = 'advanced'
        metadata['difficulty'] = difficulty
        
        return metadata
    
    def _detect_business_areas(self, content: str) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ñ–µ—Ä—ã –±–∏–∑–Ω–µ—Å–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ"""
        business_patterns = {
            '–¢–æ—Ä–≥–æ–≤–ª—è': ['–º–∞–≥–∞–∑–∏–Ω', '–º–∏–Ω–∏-–º–∞—Ä–∫–µ—Ç', '—Ç–æ—Ä–≥–æ–≤–ª—è', '–ø–æ–∫—É–ø–∞—Ç–µ–ª—å', '—Ç–æ–≤–∞—Ä', '–ø—Ä–æ–¥–∞–∂–∞'],
            '–£—Å–ª—É–≥–∏': ['–∞–≤—Ç–æ–º–æ–π–∫–∞', '—Å–µ—Ä–≤–∏—Å', '–∫–ª–∏–µ–Ω—Ç', '—É—Å–ª—É–≥–∞', '–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ'],
            'IT': ['–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', '—Å–∞–π—Ç', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '–∫–æ–¥'],
            '–ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ': ['–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ', '–∑–∞–≤–æ–¥', '–∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ', '–ø—Ä–æ–¥—É–∫—Ü–∏—è'],
            '–û–±—â–µ–ø–∏—Ç': ['–∫–∞—Ñ–µ', '—Ä–µ—Å—Ç–æ—Ä–∞–Ω', '–∫—É–ª–∏–Ω–∞—Ä–∏—è', '–µ–¥–∞', '–ø–æ–≤–∞—Ä'],
            '–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ': ['–æ–±—É—á–µ–Ω–∏–µ', '–∫—É—Ä—Å', '—Å—Ç—É–¥–µ–Ω—Ç', '–ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å']
        }
        
        detected_areas = []
        content_lower = content.lower()
        
        for area, keywords in business_patterns.items():
            if any(keyword in content_lower for keyword in keywords):
                detected_areas.append(area)
        
        return detected_areas if detected_areas else ['–û–±—â–∏–π']
    
    def _split_into_sections(self, content: str) -> Dict[str, str]:
        """–†–∞–∑–¥–µ–ª—è–µ—Ç –∫–µ–π—Å –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–µ–∫—Ü–∏–∏"""
        sections = {}
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata_match = re.search(r'### –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ(.*?)(?=\n###|\n##|$)', content, re.DOTALL)
        if metadata_match:
            sections['metadata'] = metadata_match.group(1).strip()
        
        # –°—Ü–µ–Ω–∞—Ä–∏–π –∫–µ–π—Å–∞
        scenario_match = re.search(r'### –°—Ü–µ–Ω–∞—Ä–∏–π –∫–µ–π—Å–∞\s*\n(.*?)(?=\n##|$)', content, re.DOTALL)
        if scenario_match:
            scenario_content = scenario_match.group(1).strip()
            logger.info(f"–ù–∞–π–¥–µ–Ω —Å—Ü–µ–Ω–∞—Ä–∏–π –¥–ª–∏–Ω–æ–π {len(scenario_content)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –°–æ–∑–¥–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Å–µ–∫—Ü–∏–∏
            sections['scenario'] = scenario_content
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –≤—ã–¥–µ–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —á–∞—Å—Ç–∏
            if "–ü–æ–≥—Ä—É–∂–µ–Ω–∏–µ" in scenario_content:
                immersion_match = re.search(r'#### –≠—Ç–∞–ø 1: –ü–æ–≥—Ä—É–∂–µ–Ω–∏–µ(.*?)(?=\n####|\n##|$)', scenario_content, re.DOTALL)
                if immersion_match:
                    sections['situation'] = immersion_match.group(1).strip()
            
            # –ò—â–µ–º –∑–∞–¥–∞–Ω–∏—è –∏ –≤–æ–ø—Ä–æ—Å—ã
            assignments = re.findall(r'–ó–∞–¥–∞–Ω–∏–µ \d+:(.*?)(?=–ó–∞–¥–∞–Ω–∏–µ \d+:|–û—Ç–≤–µ—Ç –õ–µ–æ|\n##|$)', scenario_content, re.DOTALL)
            if assignments:
                sections['questions'] = '\n\n'.join([f"–ó–∞–¥–∞–Ω–∏–µ {i+1}:{assignment.strip()}" for i, assignment in enumerate(assignments)])
            
            # –ò—â–µ–º –æ—Ç–≤–µ—Ç—ã –õ–µ–æ
            leo_responses = re.findall(r'–û—Ç–≤–µ—Ç –õ–µ–æ –∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ:(.*?)(?=–ó–∞–¥–∞–Ω–∏–µ \d+:|–û—Ç–≤–µ—Ç –õ–µ–æ|\n##|$)', scenario_content, re.DOTALL)
            if leo_responses:
                sections['solutions'] = '\n\n'.join([f"–û—Ç–≤–µ—Ç –õ–µ–æ:{response.strip()}" for response in leo_responses])
        else:
            logger.warning(f"–°—Ü–µ–Ω–∞—Ä–∏–π –∫–µ–π—Å–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è –∫–µ–π—Å–∞ {case_id}")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –≤—ã–¥–µ–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —á–∞—Å—Ç–∏
            if "–ü–æ–≥—Ä—É–∂–µ–Ω–∏–µ" in full_scenario:
                immersion_match = re.search(r'#### –≠—Ç–∞–ø 1: –ü–æ–≥—Ä—É–∂–µ–Ω–∏–µ(.*?)(?=\n####|\n##|$)', full_scenario, re.DOTALL)
                if immersion_match:
                    sections['situation'] = immersion_match.group(1).strip()
            
            # –ò—â–µ–º –∑–∞–¥–∞–Ω–∏—è –∏ –≤–æ–ø—Ä–æ—Å—ã
            assignments = re.findall(r'–ó–∞–¥–∞–Ω–∏–µ \d+:(.*?)(?=–ó–∞–¥–∞–Ω–∏–µ \d+:|–û—Ç–≤–µ—Ç –õ–µ–æ|\n##|$)', full_scenario, re.DOTALL)
            if assignments:
                sections['questions'] = '\n\n'.join([f"–ó–∞–¥–∞–Ω–∏–µ {i+1}:{assignment.strip()}" for i, assignment in enumerate(assignments)])
            
            # –ò—â–µ–º –æ—Ç–≤–µ—Ç—ã –õ–µ–æ
            leo_responses = re.findall(r'–û—Ç–≤–µ—Ç –õ–µ–æ –∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ:(.*?)(?=–ó–∞–¥–∞–Ω–∏–µ \d+:|–û—Ç–≤–µ—Ç –õ–µ–æ|\n##|$)', full_scenario, re.DOTALL)
            if leo_responses:
                sections['solutions'] = '\n\n'.join([f"–û—Ç–≤–µ—Ç –õ–µ–æ:{response.strip()}" for response in leo_responses])
        
        return sections
    
    def create_chunks(self, case_data: Dict[str, Any]) -> List[CaseChunk]:
        """–°–æ–∑–¥–∞–µ—Ç —á–∞–Ω–∫–∏ –∏–∑ –¥–∞–Ω–Ω—ã—Ö –∫–µ–π—Å–∞"""
        chunks = []
        
        case_id = case_data['case_id']
        title = case_data['title']
        metadata = case_data['metadata']
        sections = case_data['sections']
        
        # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —Å–µ–∫—Ü–∏–∏
        for section_type, content in sections.items():
            if content and len(content.strip()) > 100:  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
                # –†–∞–∑–±–∏–≤–∞–µ–º –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –±–æ–ª–µ–µ –º–µ–ª–∫–∏–µ —á–∞–Ω–∫–∏
                if len(content) > 2000:
                    # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∞–±–∑–∞—Ü–∞–º –∏–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
                    paragraphs = re.split(r'\n\n+', content)
                    for i, paragraph in enumerate(paragraphs):
                        if len(paragraph.strip()) > 100:
                            chunk = CaseChunk(
                                case_id=case_id,
                                chunk_type=f"{section_type}_part_{i+1}",
                                title=title,
                                content=paragraph.strip(),
                                after_level=metadata['after_level'],
                                skill_name=metadata['skill_name'],
                                tags=metadata['tags'],
                                business_areas=metadata['business_areas'],
                                difficulty=metadata['difficulty']
                            )
                            chunks.append(chunk)
                else:
                    chunk = CaseChunk(
                        case_id=case_id,
                        chunk_type=section_type,
                        title=title,
                        content=content.strip(),
                        after_level=metadata['after_level'],
                        skill_name=metadata['skill_name'],
                        tags=metadata['tags'],
                        business_areas=metadata['business_areas'],
                        difficulty=metadata['difficulty']
                    )
                    chunks.append(chunk)
        
        return chunks
    
    def generate_embeddings(self, chunks: List[CaseChunk]) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —á–∞–Ω–∫–æ–≤"""
        documents = []
        
        logger.info(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(chunks)} —á–∞–Ω–∫–æ–≤...")
        
        for i, chunk in enumerate(chunks):
            try:
                # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
                embedding_text = f"{chunk.title}\n\n{chunk.content}"
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
                response = self.openai_client.embeddings.create(
                    input=embedding_text,
                    model=self.embedding_model
                )
                
                embedding = response.data[0].embedding
                
                # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä
                doc_id = str(uuid.uuid4())
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
                document = {
                    'id': doc_id,
                    'content': chunk.content,
                    'embedding': embedding,
                    'metadata': {
                        'source': 'bizlevel_case',
                        'case_id': chunk.case_id,
                        'case_title': chunk.title,
                        'chunk_type': chunk.chunk_type,
                        'level_id': chunk.after_level,
                        'skill_name': chunk.skill_name,
                        'tags': chunk.tags,
                        'business_areas': chunk.business_areas,
                        'difficulty': chunk.difficulty
                    }
                }
                
                documents.append(document)
                
                if (i + 1) % 10 == 0:
                    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1}/{len(chunks)} —á–∞–Ω–∫–æ–≤")
                
                # –ü–∞—É–∑–∞ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è rate limit
                time.sleep(0.1)
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —á–∞–Ω–∫–∞ {i}: {e}")
                continue
        
        logger.info(f"–£—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏")
        return documents
    
    def save_to_database(self, documents: List[Dict[str, Any]]) -> bool:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö Supabase"""
        try:
            logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...")
            
            # –£–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–µ–π—Å—ã –∏–∑ –±–∞–∑—ã
            delete_result = self.supabase.table('documents').delete().eq('metadata->>source', 'bizlevel_case').execute()
            logger.info(f"–£–¥–∞–ª–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∫–µ–π—Å–æ–≤: {len(delete_result.data) if delete_result.data else 0}")
            
            # –í—Å—Ç–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –±–∞—Ç—á–∞–º–∏
            batch_size = 50
            for i in range(0, len(documents), batch_size):
                batch = documents[i:i + batch_size]
                
                result = self.supabase.table('documents').insert(batch).execute()
                
                if result.data:
                    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω –±–∞—Ç—á {i//batch_size + 1}: {len(result.data)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                else:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –±–∞—Ç—á–∞ {i//batch_size + 1}")
                    return False
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
                time.sleep(0.5)
            
            logger.info("‚úÖ –í—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö: {e}")
            return False
    
    def index_cases(self, markdown_file: str) -> bool:
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∫–µ–π—Å–æ–≤"""
        try:
            logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –∫–µ–π—Å–æ–≤ BizLevel")
            
            # 1. –ü–∞—Ä—Å–∏–Ω–≥ –∫–µ–π—Å–æ–≤
            logger.info(f"üìñ –ü–∞—Ä—Å–∏–Ω–≥ –∫–µ–π—Å–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞: {markdown_file}")
            cases = self.parse_cases_from_markdown(markdown_file)
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ –∫–µ–π—Å–æ–≤: {len(cases)}")
            
            if not cases:
                logger.error("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –∫–µ–π—Å–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
                return False
            
            # 2. –°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤
            logger.info("‚úÇÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤...")
            all_chunks = []
            for case in cases:
                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–µ–π—Å–∞ {case['case_id']}: {case['title']}")
                logger.info(f"–°–µ–∫—Ü–∏–∏: {list(case['sections'].keys())}")
                for section_type, content in case['sections'].items():
                    logger.info(f"  {section_type}: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
                
                chunks = self.create_chunks(case)
                logger.info(f"  –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
                all_chunks.extend(chunks)
            
            logger.info(f"–°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(all_chunks)}")
            
            if not all_chunks:
                logger.error("–ù–µ —Å–æ–∑–¥–∞–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞")
                return False
            
            # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            documents = self.generate_embeddings(all_chunks)
            
            if not documents:
                logger.error("–ù–µ —Å–æ–∑–¥–∞–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º")
                return False
            
            # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É
            success = self.save_to_database(documents)
            
            if success:
                logger.info("üéâ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∫–µ–π—Å–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
                logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
                logger.info(f"  - –ö–µ–π—Å–æ–≤: {len(cases)}")
                logger.info(f"  - –ß–∞–Ω–∫–æ–≤: {len(all_chunks)}")
                logger.info(f"  - –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ: {len(documents)}")
                return True
            else:
                logger.error("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")
                return False
                
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")
            return False

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫—Ä–∏–ø—Ç–∞"""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    required_vars = ['OPENAI_API_KEY', 'SUPABASE_URL', 'SUPABASE_SERVICE_ROLE_KEY']
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    
    if missing_vars:
        logger.error(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è: {', '.join(missing_vars)}")
        return False
    
    # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –∫–µ–π—Å–∞–º–∏
    script_dir = Path(__file__).parent
    cases_file = script_dir.parent / 'docs' / 'bizlevel-cases-scenarios.md'
    
    if not cases_file.exists():
        logger.error(f"–§–∞–π–ª —Å –∫–µ–π—Å–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {cases_file}")
        return False
    
    # –ó–∞–ø—É—Å–∫ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
    indexer = CaseIndexer()
    success = indexer.index_cases(str(cases_file))
    
    return success

if __name__ == '__main__':
    success = main()
    exit(0 if success else 1)
