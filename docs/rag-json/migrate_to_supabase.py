#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –º–∏–≥—Ä–∞—Ü–∏–∏ JSON –¥–∞–Ω–Ω—ã—Ö –≤ Supabase
"""

import json
import os
import asyncio
import aiohttp
from pathlib import Path
from typing import List, Dict, Any
import hashlib
from datetime import datetime

class SupabaseMigrator:
    def __init__(self, supabase_url: str, supabase_key: str, openai_api_key: str):
        self.supabase_url = supabase_url
        self.supabase_key = supabase_key
        self.openai_api_key = openai_api_key
        self.session = None
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def create_tables(self):
        """–°–æ–∑–¥–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ç–∞–±–ª–∏—Ü—ã –≤ Supabase"""
        
        # DDL –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü
        ddl_queries = [
            """
            -- –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è —Ñ–∞–∫—Ç–æ–≤ —É—Ä–æ–∫–æ–≤
            CREATE TABLE IF NOT EXISTS lesson_facts (
                id TEXT PRIMARY KEY,
                content TEXT NOT NULL,
                lesson_id INTEGER NOT NULL,
                level_number INTEGER NOT NULL,
                section TEXT NOT NULL,
                title TEXT NOT NULL,
                file_name TEXT NOT NULL,
                doc_id TEXT NOT NULL,
                chunk_index INTEGER NOT NULL,
                tags TEXT[] DEFAULT '{}',
                topics TEXT[] DEFAULT '{}',
                keywords TEXT[] DEFAULT '{}',
                embedding VECTOR(1536), -- OpenAI text-embedding-3-small
                created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
            );
            """,
            """
            -- –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            CREATE INDEX IF NOT EXISTS idx_lesson_facts_level_number ON lesson_facts(level_number);
            CREATE INDEX IF NOT EXISTS idx_lesson_facts_section ON lesson_facts(section);
            CREATE INDEX IF NOT EXISTS idx_lesson_facts_lesson_id ON lesson_facts(lesson_id);
            CREATE INDEX IF NOT EXISTS idx_lesson_facts_tags ON lesson_facts USING GIN(tags);
            CREATE INDEX IF NOT EXISTS idx_lesson_facts_topics ON lesson_facts USING GIN(topics);
            CREATE INDEX IF NOT EXISTS idx_lesson_facts_keywords ON lesson_facts USING GIN(keywords);
            """,
            """
            -- –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ —Å—Ö–æ–¥—Å—Ç–≤—É
            CREATE INDEX IF NOT EXISTS idx_lesson_facts_embedding ON lesson_facts 
            USING ivfflat (embedding vector_cosine_ops) 
            WITH (lists = 100);
            """,
            """
            -- –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —É—Ä–æ–∫–æ–≤
            CREATE TABLE IF NOT EXISTS lesson_metadata (
                lesson_id INTEGER PRIMARY KEY,
                level_id INTEGER NOT NULL,
                title TEXT NOT NULL,
                description TEXT NOT NULL,
                video_url TEXT,
                duration_minutes INTEGER,
                language TEXT NOT NULL,
                version TEXT NOT NULL,
                created_at TIMESTAMP WITH TIME ZONE NOT NULL,
                updated_at TIMESTAMP WITH TIME ZONE NOT NULL,
                checksum_sha256 TEXT NOT NULL,
                content JSONB NOT NULL
            );
            """,
            """
            -- –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            CREATE INDEX IF NOT EXISTS idx_lesson_metadata_level_id ON lesson_metadata(level_id);
            CREATE INDEX IF NOT EXISTS idx_lesson_metadata_language ON lesson_metadata(language);
            CREATE INDEX IF NOT EXISTS idx_lesson_metadata_version ON lesson_metadata(version);
            """
        ]
        
        print("–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –≤ Supabase...")
        for i, query in enumerate(ddl_queries, 1):
            try:
                await self.execute_sql(query)
                print(f"‚úÖ –ó–∞–ø—Ä–æ—Å {i}/{len(ddl_queries)} –≤—ã–ø–æ–ª–Ω–µ–Ω")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∑–∞–ø—Ä–æ—Å–µ {i}: {e}")
                raise
    
    async def execute_sql(self, query: str):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç SQL –∑–∞–ø—Ä–æ—Å –≤ Supabase"""
        url = f"{self.supabase_url}/rest/v1/rpc/exec_sql"
        headers = {
            "apikey": self.supabase_key,
            "Authorization": f"Bearer {self.supabase_key}",
            "Content-Type": "application/json"
        }
        
        async with self.session.post(url, json={"query": query}, headers=headers) as response:
            if response.status != 200:
                error_text = await response.text()
                raise Exception(f"SQL execution failed: {response.status} - {error_text}")
            return await response.json()
    
    async def get_embedding(self, text: str) -> List[float]:
        """–ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –æ—Ç OpenAI"""
        url = "https://api.openai.com/v1/embeddings"
        headers = {
            "Authorization": f"Bearer {self.openai_api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "input": text,
            "model": "text-embedding-3-small"
        }
        
        async with self.session.post(url, json=data, headers=headers) as response:
            if response.status != 200:
                error_text = await response.text()
                raise Exception(f"OpenAI API error: {response.status} - {error_text}")
            
            result = await response.json()
            return result["data"][0]["embedding"]
    
    async def load_lesson_metadata(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —É—Ä–æ–∫–æ–≤"""
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —É—Ä–æ–∫–æ–≤...")
        
        lesson_files = []
        for root, dirs, files in os.walk('levels'):
            for file in files:
                if file == 'lesson.json':
                    lesson_files.append(os.path.join(root, file))
        
        for lesson_file in lesson_files:
            try:
                with open(lesson_file, 'r', encoding='utf-8') as f:
                    lesson_data = json.load(f)
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
                metadata = {
                    "lesson_id": lesson_data['lesson_id'],
                    "level_id": lesson_data['level_id'],
                    "title": lesson_data['title'],
                    "description": lesson_data['description'],
                    "video_url": lesson_data.get('video_url'),
                    "duration_minutes": lesson_data.get('duration_minutes'),
                    "language": lesson_data['language'],
                    "version": lesson_data['version'],
                    "created_at": lesson_data['created_at'],
                    "updated_at": lesson_data['updated_at'],
                    "checksum_sha256": lesson_data['checksum_sha256'],
                    "content": lesson_data['content']
                }
                
                # –í—Å—Ç–∞–≤–ª—è–µ–º –≤ –±–∞–∑—É
                await self.insert_lesson_metadata(metadata)
                print(f"‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —É—Ä–æ–∫–∞ {lesson_data['lesson_id']} –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {lesson_file}: {e}")
    
    async def insert_lesson_metadata(self, metadata: Dict[str, Any]):
        """–í—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —É—Ä–æ–∫–∞ –≤ –±–∞–∑—É"""
        url = f"{self.supabase_url}/rest/v1/lesson_metadata"
        headers = {
            "apikey": self.supabase_key,
            "Authorization": f"Bearer {self.supabase_key}",
            "Content-Type": "application/json",
            "Prefer": "resolution=merge-duplicates"
        }
        
        async with self.session.post(url, json=metadata, headers=headers) as response:
            if response.status not in [200, 201]:
                error_text = await response.text()
                raise Exception(f"Failed to insert lesson metadata: {response.status} - {error_text}")
    
    async def load_facts(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∞–∫—Ç—ã —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏"""
        print("–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–∫—Ç–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏...")
        
        fact_files = []
        for root, dirs, files in os.walk('levels'):
            for file in files:
                if file == 'facts.jsonl':
                    fact_files.append(os.path.join(root, file))
        
        total_facts = 0
        for fact_file in fact_files:
            try:
                with open(fact_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            fact = json.loads(line)
                            
                            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
                            embedding = await self.get_embedding(fact['content'])
                            fact['embedding'] = embedding
                            
                            # –í—Å—Ç–∞–≤–ª—è–µ–º –≤ –±–∞–∑—É
                            await self.insert_fact(fact)
                            total_facts += 1
                            
                            if total_facts % 10 == 0:
                                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {total_facts} —Ñ–∞–∫—Ç–æ–≤...")
                
                print(f"‚úÖ –§–∞–π–ª {fact_file} –æ–±—Ä–∞–±–æ—Ç–∞–Ω")
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {fact_file}: {e}")
        
        print(f"üéâ –í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {total_facts} —Ñ–∞–∫—Ç–æ–≤")
    
    async def insert_fact(self, fact: Dict[str, Any]):
        """–í—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ–∞–∫—Ç –≤ –±–∞–∑—É"""
        url = f"{self.supabase_url}/rest/v1/lesson_facts"
        headers = {
            "apikey": self.supabase_key,
            "Authorization": f"Bearer {self.supabase_key}",
            "Content-Type": "application/json",
            "Prefer": "resolution=merge-duplicates"
        }
        
        async with self.session.post(url, json=fact, headers=headers) as response:
            if response.status not in [200, 201]:
                error_text = await response.text()
                raise Exception(f"Failed to insert fact: {response.status} - {error_text}")
    
    async def create_search_functions(self):
        """–°–æ–∑–¥–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        search_functions = [
            """
            -- –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            CREATE OR REPLACE FUNCTION search_lesson_facts(
                query_text TEXT,
                query_embedding VECTOR(1536),
                level_filter INTEGER DEFAULT NULL,
                section_filter TEXT DEFAULT NULL,
                limit_count INTEGER DEFAULT 10
            )
            RETURNS TABLE (
                id TEXT,
                content TEXT,
                lesson_id INTEGER,
                level_number INTEGER,
                section TEXT,
                title TEXT,
                similarity_score FLOAT,
                bm25_score FLOAT,
                combined_score FLOAT
            )
            LANGUAGE plpgsql
            AS $$
            BEGIN
                RETURN QUERY
                SELECT 
                    lf.id,
                    lf.content,
                    lf.lesson_id,
                    lf.level_number,
                    lf.section,
                    lf.title,
                    (1 - (lf.embedding <=> query_embedding)) as similarity_score,
                    ts_rank(to_tsvector('russian', lf.content), plainto_tsquery('russian', query_text)) as bm25_score,
                    (0.7 * (1 - (lf.embedding <=> query_embedding)) + 0.3 * ts_rank(to_tsvector('russian', lf.content), plainto_tsquery('russian', query_text))) as combined_score
                FROM lesson_facts lf
                WHERE 
                    (level_filter IS NULL OR lf.level_number = level_filter)
                    AND (section_filter IS NULL OR lf.section = section_filter)
                ORDER BY combined_score DESC
                LIMIT limit_count;
            END;
            $$
            """,
            """
            -- –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ —É—Ä–æ–≤–Ω—è–º
            CREATE OR REPLACE FUNCTION search_by_level(
                level_number INTEGER,
                query_text TEXT DEFAULT '',
                limit_count INTEGER DEFAULT 20
            )
            RETURNS TABLE (
                id TEXT,
                content TEXT,
                lesson_id INTEGER,
                section TEXT,
                title TEXT
            )
            LANGUAGE plpgsql
            AS $$
            BEGIN
                IF query_text = '' THEN
                    RETURN QUERY
                    SELECT lf.id, lf.content, lf.lesson_id, lf.section, lf.title
                    FROM lesson_facts lf
                    WHERE lf.level_number = level_number
                    ORDER BY lf.lesson_id, lf.chunk_index
                    LIMIT limit_count;
                ELSE
                    RETURN QUERY
                    SELECT lf.id, lf.content, lf.lesson_id, lf.section, lf.title
                    FROM lesson_facts lf
                    WHERE lf.level_number = level_number
                        AND to_tsvector('russian', lf.content) @@ plainto_tsquery('russian', query_text)
                    ORDER BY ts_rank(to_tsvector('russian', lf.content), plainto_tsquery('russian', query_text)) DESC
                    LIMIT limit_count;
                END IF;
            END;
            $$
            """
        ]
        
        print("–°–æ–∑–¥–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π –ø–æ–∏—Å–∫–∞...")
        for i, func in enumerate(search_functions, 1):
            try:
                await self.execute_sql(func)
                print(f"‚úÖ –§—É–Ω–∫—Ü–∏—è {i}/{len(search_functions)} —Å–æ–∑–¥–∞–Ω–∞")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ {i}: {e}")

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –º–∏–≥—Ä–∞—Ü–∏–∏"""
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    supabase_url = os.getenv('SUPABASE_URL')
    supabase_key = os.getenv('SUPABASE_ANON_KEY')
    openai_api_key = os.getenv('OPENAI_API_KEY')
    
    if not all([supabase_url, supabase_key, openai_api_key]):
        print("‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è:")
        print("SUPABASE_URL, SUPABASE_ANON_KEY, OPENAI_API_KEY")
        return
    
    async with SupabaseMigrator(supabase_url, supabase_key, openai_api_key) as migrator:
        try:
            # 1. –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã
            await migrator.create_tables()
            
            # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —É—Ä–æ–∫–æ–≤
            await migrator.load_lesson_metadata()
            
            # 3. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–∫—Ç—ã —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
            await migrator.load_facts()
            
            # 4. –°–æ–∑–¥–∞–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ–∏—Å–∫–∞
            await migrator.create_search_functions()
            
            print("üéâ –ú–∏–≥—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –º–∏–≥—Ä–∞—Ü–∏–∏: {e}")

if __name__ == "__main__":
    asyncio.run(main())
