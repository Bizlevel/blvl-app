# 1. Анализ архитектуры и логики чата Leo AI  (04.08.2025)

## 1. Стек файлов и компонентов  
- `lib/services/leo_service.dart` — клиент-сервис, инкапсулирующий:  
  – отправку сообщений (Edge Function / прямой OpenAI API);  
  – модерацию через OpenAI /moderations;  
  – учёт лимитов, RPC `decrement_leo_message`, `reset_leo_unread`;  
  – сохранение чата и сообщений в БД (`leo_chats`, `leo_messages`).  
• Провайдеры Riverpod  
  – `leoServiceProvider.dart` (DI SupabaseClient → LeoService)  
  – `leo_unread_provider.dart` — стрим `unread_count` по чату.  
• UI  
  – `FloatingChatBubble.dart` — «пузырик» с бейджем непрочитанных; открывает `LeoDialogScreen`.  
  – `leo_chat_screen.dart` — список чатов.  
  – `leo_dialog_screen.dart` — сам диалог: пагинация, ввод, автоскролл, вызовы LeoService.  
  – вспом. виджеты `ChatItem`, `LeoMessageBubble`.  
• Серверная логика  
  – `supabase/functions/leo-chat/index.ts` — Edge Function-прокси к OpenAI.  
    Использует Service-Role key для БД, добавляет CORS-заголовки.  

## 2. Формирование контекста для Leo  
• Клиент собирает `messages` в формате Chat Completion (`role/user/assistant`, `content`).  
  – Начальный контекст – история из `leo_messages` (пагинация 30 шт.).  
  – Системного промпта со стороны клиента нет (параметр `systemPrompt` в `FloatingChatBubble` сейчас никуда не передаётся).  
• Edge Function добавляет системовое сообщение:  
```text
role: "system"
content: "Ты Leo — дружелюбный AI-ментор по бизнесу. Отвечай лаконично на русском языке. Имя пользователя: … Цель: … О себе: …"
```  
  – Персональные данные берутся из таблицы `users` (`name`, `about`, `goal`) через `supabaseAdmin.auth.getUser(jwt)` + select.  
• Если в `.env` определён `OPENAI_API_KEY`, мобильный клиент пропускает Edge Function и обращается к OpenAI напрямую (та же модель gpt-3.5-turbo, temperature 0.7).  

## 3. Формат ответа и обработка  
• Ожидается JSON вида  
```json
{
  "message": { "content": "<ответ>" },
  "usage": { "prompt_tokens":..., "completion_tokens":..., "total_tokens":... }
}
```  
• UI извлекает `message.content` и отображает plain-text; markdown/форматирование не требуют-ся.  
• После приёма ответа:  
  – LeoService сохраняет в `leo_messages` (`role='assistant'`), инкрементирует `message_count` в `leo_chats`.  
  – Счётчик непрочитанных (`unread_count`) поддерживается Postgres-триггером (не в коде клиента).  

## 4. Модель данных Supabase (public schema)  
• `leo_chats`  
  – `id uuid PK`, `user_id uuid FK users`, `title text`, `message_count int`,  
    `unread_count int`, `created_at / updated_at timestamptz`.  
• `leo_messages`  
  – `id uuid PK`, `chat_id uuid FK leo_chats`, `user_id uuid FK users`,  
    `role text`, `content text`, `token_count int`, `created_at timestamptz`.  
• Таблица `users` содержит лимиты:  
  – `leo_messages_total` (для free-тарифа; дефолт 30);  
  – `leo_messages_today`, `leo_reset_at` (дневной лимит premium);  
  – `is_premium bool`.  
• Сторонние процедуры/триггеры  
  – `decrement_leo_message()` — атомарно уменьшает счётчик и возвращает остаток.  
  – `reset_leo_unread(p_chat_id uuid)` — обнуляет `unread_count` чата.  
  – триггер на `leo_messages` ↑ `unread_count`, заполняет `token_count` (по usage).  

## 5. Поток взаимодействия  
1. Пользователь открывает `LeoDialogScreen` → загружается первая страница сообщений; LeoService `reset_unread`.  
2. Пользователь вводит сообщение →  
   а) LeoService `saveConversation(role:'user')`;  
   б) RPC `decrement_leo_message` → обновляет лимит;  
   в) `sendMessage` → (Edge Function или прямой OpenAI) → ответ;  
   г) LeoService `saveConversation(role:'assistant')`.  
3. UI рендерит новое сообщение, счётчики на экранах обновляются по real-time стримам.  

## 6. Итог:  
– Вся логика Leo сконцентрирована в Edge Function + `LeoService`.  
– Контекст включает персональные данные пользователя; связь с уроками/уровнями пока отсутствует (но параметр `systemPrompt` в `FloatingChatBubble` заложен для будущей передачи контекста урока).  
– Формат общения — стандартный Chat Completion, ответы на русском, лаконичные.  
– Supabase хранит чаты/сообщения, лимиты и статистику в `users`.  

# 2. Сейчас в коде реализованы две схемы общения с OpenAI.

## 1. Edge Function `supabase/functions/leo-chat/index.ts`  
   – Запрос приходит с Bearer-JWT пользователя, сервер сам достаёт профиль и добавляет системный prompt.  
   – Ключ `OPENAI_API_KEY` хранится **только** в переменных окружения Supabase; до клиента он не попадает.  
   – Ответ возвращается клиенту; токены/непрочитанные считаются триггерами в БД.  
   → Это безопасный, “правильный” путь.

## 2. Клиентский обход в `lib/services/leo_service.dart`  
```dart
final openaiKey = envOrDefine('OPENAI_API_KEY');
if (openaiKey.isNotEmpty) {
    // прямой запрос к https://api.openai.com
}
```
   – Если при сборке передан `--dart-define OPENAI_API_KEY=…` **или** ключ есть в `.env`, Flutter-клиент инжектирует его в бинарь и начинает обращаться в OpenAI напрямую, обходя функцию-прокси.  
   – Контекст пользователя формируется только из локальной истории сообщений, без профиля из БД.  
   – Ключ становится доступен реверс-инжинированием → нарушение безопасности.
## Фактическое поведение сейчас:  
• В mobile/web сборках, где `.env` читается, ключ присутствует → используется **прямая клиентская схема**.  
• Если ключ не передавать (prod Web через Vercel, CI без секретов) – клиент падает на Edge Function.
## Вывод  
Рабочей (целевой) должна быть **серверная (Edge Function)** логика; клиентская нужна только как “костыль” для локальной разработки, но приводит к утечке API-ключа.
## Рекомендации  
1. Удалить/закомментировать ветку с прямым вызовом OpenAI из `LeoService`:  
   – проверку `openaiKey.isNotEmpty` и последующий блок 80-120 строк.  
2. Оставить только вызов Edge Function.  
3. В `.env` хранить сервис-роль ключ Supabase и (опционально) OpenAI, но **не пробрасывать** его в сборку Flutter (`--dart-define` не передавать).  
4. Добавить unit-тест, убеждающийся, что при наличии `OPENAI_API_KEY` клиент всё-равно использует `/functions/v1/leo-chat`.